# ðŸ”’ Federated Learning with Differential Privacy

A privacy-preserving machine learning pipeline that trains models across decentralized data sources without exposing raw data. Implements **Federated Averaging (FedAvg)** with **Differential Privacy (DP)** guarantees using gradient clipping and calibrated Gaussian noise injection.

##  Project Overview

This project demonstrates how organizations can collaboratively train ML models while:
- **Never sharing raw data** between participants (federated learning)
- **Bounding information leakage** from model updates (differential privacy)
- **Tracking privacy budgets** (Îµ, Î´) across training rounds
- **Quantifying the privacyâ€“utility tradeoff** empirically

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client 1   â”‚  â”‚   Client 2   â”‚  â”‚   Client N   â”‚
â”‚  Local Data  â”‚  â”‚  Local Data  â”‚  â”‚  Local Data  â”‚
â”‚  Local Train â”‚  â”‚  Local Train â”‚  â”‚  Local Train â”‚
â”‚  + DP Noise  â”‚  â”‚  + DP Noise  â”‚  â”‚  + DP Noise  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â”‚    Noised Gradients Only          â”‚
       â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Aggregation Server            â”‚
    â”‚  FedAvg: weighted average of updates    â”‚
    â”‚  Privacy Accountant: track (Îµ, Î´)       â”‚
    â”‚  Global Model Update                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##  Key Concepts

| Concept | Implementation |
|---|---|
| **Federated Averaging** | Weighted aggregation of client model updates |
| **DP-SGD** | Per-sample gradient clipping + Gaussian noise |
| **Privacy Accountant** | RÃ©nyi Differential Privacy (RDP) â†’ (Îµ, Î´) conversion |
| **Non-IID Data** | Dirichlet-based data partitioning across clients |
| **Membership Inference Attack** | Shadow model attack to empirically validate DP protection |

##  Installation

```bash
# Clone the repository
git clone https://github.com/acRei/federated-learning-dp.git
cd federated-learning-dp

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

##  Start

### 1. Run the Full Pipeline

```bash
python src/main.py
```

This will:
- Partition MNIST across 5 simulated clients (non-IID)
- Train a CNN using Federated Averaging + DP-SGD
- Track privacy budget (Îµ) consumption per round
- Output accuracy vs. privacy tradeoff metrics
- Run a membership inference attack to validate DP protection

### 2. Customize Parameters

```bash
python src/main.py \
  --num_clients 10 \
  --num_rounds 50 \
  --local_epochs 5 \
  --noise_multiplier 1.0 \
  --max_grad_norm 1.0 \
  --target_epsilon 8.0 \
  --target_delta 1e-5
```

### 3. Launch the Dashboard

```bash
python src/dashboard.py
```

Opens an interactive visualization showing training progress, per-round Îµ accumulation, and attack success rates.

## ðŸ“ Project Structure

```
federated-learning-dp/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Entry point & orchestrator
â”‚   â”œâ”€â”€ model.py             # CNN architecture
â”‚   â”œâ”€â”€ client.py            # Federated client with DP-SGD
â”‚   â”œâ”€â”€ server.py            # Aggregation server + FedAvg
â”‚   â”œâ”€â”€ privacy.py           # RDP accountant & noise calibration
â”‚   â”œâ”€â”€ data.py              # Non-IID data partitioning
â”‚   â”œâ”€â”€ attacks.py           # Membership inference attack
â”‚   â””â”€â”€ dashboard.py         # Results visualization (HTML)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_privacy.py      # Unit tests for privacy guarantees
â””â”€â”€ notebooks/
    â””â”€â”€ analysis.ipynb        # Experiment analysis notebook
```

##  Results

Typical results on MNIST with 5 clients, 30 rounds:

| Configuration | Test Accuracy | Final Îµ (Î´=1e-5) | MIA AUC |
|---|---|---|---|
| No DP (baseline) | 98.2% | âˆž | 0.62 |
| DP (Ïƒ=0.5) | 96.8% | 12.4 | 0.54 |
| DP (Ïƒ=1.0) | 94.1% | 4.7 | 0.51 |
| DP (Ïƒ=2.0) | 88.3% | 1.9 | 0.50 |

> **MIA AUC = 0.50** means the attacker performs no better than random guessing â†’ strong privacy.

##  Key Takeaways

1. **Differential privacy provably bounds information leakage** â€” even a well-resourced attacker cannot reliably determine if a specific sample was in the training set.
2. **Privacy comes at a utility cost** â€” higher noise (lower Îµ) reduces accuracy, requiring careful calibration.
3. **Federated learning alone is not sufficient** â€” without DP, gradient updates can leak sensitive information via model inversion or membership inference attacks.
4. **RDP composition provides tighter bounds** than naÃ¯ve composition, enabling more training rounds within a privacy budget.

##  References

- McMahan et al., *Communication-Efficient Learning of Deep Networks from Decentralized Data* (2017)
- Abadi et al., *Deep Learning with Differential Privacy* (2016)
- Mironov, *RÃ©nyi Differential Privacy* (2017)
- Shokri et al., *Membership Inference Attacks Against Machine Learning Models* (2017)

## License

MIT
